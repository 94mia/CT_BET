---------------------------
Reviewers' comments:

Reviewer #1: In this paper, the authors present validation of brain extraction from CT images using the brain extraction tool (BET) of FSL. With their optimised parameters (FI and smoothing), good agreement between manual extraction and the automated technique is achieved based on the metrics of accuracy, sensitivity, specificity and DSC.

The authors should address the following points:
1)      Why was the gantry tilt correction necessary? Does this not result in degraded slices given the much higher inter-slice spacing compared with the in-slice voxel size?

- Gantry tilt was not specifically necessary and this is noted.  We perform this for our general pipeline, which involves rigid body registration of the skull-stripped images, which requires gantry tilt correction.   No interpolation is done in our process, the image is only shifted with zero padding to offset the tilt (all scans were acquired axially).
FOV problem

2)      It is indicated that the slice thickness can vary across the volume. However, in section 2.3, it is indicated that the thickness is taken from the first slice and then assumed to be homogenous throughout the image (I presume image volume). What was the slice thickness used for? What effect did the assumption of homogenous slice thickness have gin studies which had varying slice thickness throughout the study? Was a homogenous slice thickness assumed for the tilt correction which could result in distortion?
- When using the image to calculate the volume  


3)      Was the Gaussian filter applied in 2D ie a slice at a time or in 3D?
- 3D - this has been added to the text 
4)      Given the very small differences between the FI 0.01 and 0.1 and the spread shown in figure 2, it is a bit surprising that the differences reached the high level of significance shown. But, regardless, as stated in the paper, the differences are inconsequential.
- The spread of the data given in Figure 2 indicate why this reaches level of signifiance. 
5)      Some comment is perhaps warranted why in these studies the FI is quite small. A small FI gives little weight to local maxima, with the threshold dominated by the threshold determined from the global intensity histogram, based on details given in reference [5]. This seems a bit counter intuitive so some discussion on this is warranted.




Reviewer #2: The manuscript by Muschelli et al., titled "Validated Automatic Brain Extraction of Head CT Images", presents an evaluation of the FSL Brain Extraction Tool (BET) on the clinical population. It is a concise and well-written manuscript; however, I do not recommend it for publication in Neuroimage because it lacks novelty.

The authors state that the novelty of the manuscript is in the comparison of automatic segmentation (carried out by BET) to manual segmentation (carried out by "one expert reader", according to the Methods, and by several "expert CT readers", according to the Abstract). It is true that neither the Rorden et al. (2012) or the Solomon et al. (2007) paper carry out such a validation; however, it has been carried out in the original BET paper by Smith (2002), albeit on healthy subjects.

 - Smith (2002) carried out the validation on MRI scans, not CT scans, the modality which we would like to use BET

The result of the validation presented in the manuscript is that BET works very well on the 16 patients used in the study. This seems somewhat underwhelming to me, because BET is an immensely popular tool that has been heavily used for more than 10 years. It is comforting that the authors did not find any significant problems with it, but, in my opinion, this result is not novel enough to merit a publication, because the authors do not provide any novel method or methodological improvement over the existing BET algorithm.

- We wish to only provide a framework for adapting BET, which has overwhelmingly been used for MRI, for CT images.  We have found that smoothing the images prior to using BET improves performance.
 
A novel finding stated in the manuscript is the result that smoothing the data prior to BET is better than no smoothing. While novel, this is does not seem very relevant to the field, because Rorden et al. (2012) have already used smoothing in their method.

Rorden et al. (2012) does not indicate they run BET on smoothed images:  "To emulate the proprietary ABLe method, we used BET (Smith, 2002) and AIR 3.08 (Woods et al., 1998) to replicate the ABLe technique (Solomon et al., 2007). We started by setting all voxels with a Houns- field brightness greater than 100 (centers of bone and choroid plex- us) to match the dark intensity of air. Next, we manually adjusted the intensity of each image to provide a clear 8-bit grayscale image of the soft-tissue and then scalp-stripped the images using BET with a fractional intensity threshold of 0.35."  
  
Muschelli et al. vary the BET threshold of fractional integrity (0.01, 0.1, and 0.35), and suggest that the 0.01 and 0.1 thresholds provide good brain extraction; however, they also show the threshold
used by Rorden et al. (2012), which is 0.35, works just as well on the smoothed data.
 
 - We agree that 0.35 works just as well on the smoothed data for those presented in the validation, but we see that the failure rate is much higher in the ICC analysis.
 
By the way, the authors state that the 0.35 threshold was "recommended in Rorden et al., 2012"; this is not entirely correct, because the Rorden et al. paper does not explicitly advocate this value, it is simply the only threshold value they report using.
 
- We agree and the wording has been changed in the text.  Says "used"

The sample used in the study is, in my opinion, too small to provide sufficient power. There are 19 scans, but only 16 patients; were some patients scanned twice, or thrice?

- These duplicate scans were removed and additional, independent patients, were added.
 
Were these patients analyzed as independent data points, which is a bad methodological choice?
- The multiple scans were removed.

 
Also, the scans were obtained using 3 different scanners (Siemens, GE and Philips); the authors do not look into the the possible impact of the scanner, presumably because the sample size is so small that the difference across scanners cannot be studied adequately.

- As Hounsfield units are assumed standardized, we do not believe there to be a large scanner effect.  We also do not have information as to the subtype of scanner that was used.  However, we have examined this in the ICC analysis (as supplemental material).

 
In addition, the p-values from the Wilcoxon signed-rank test are not corrected for multiple comparisons.

- We agree that multiple comparisons were not taken into account, but we believe the estimates of performance are high and therefore statistical significance is secondary to the estimated effects of performance.