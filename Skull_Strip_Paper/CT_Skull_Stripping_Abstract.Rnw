\documentclass[10pt]{article}
\usepackage[top=0.5in, left=0.5in, right=0.5in, bottom=0.5in]{geometry}

\usepackage{float, amsmath}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows}
\usetikzlibrary{positioning}
\usepackage{float, amsmath}

\usepackage[hyphens]{url}
\usepackage{enumerate}
\usepackage[sort, numbers]{natbib}
\usepackage{sidecap}

\setlength{\bibsep}{0.0pt}
%\usepackage[
%  natbib = true,
%    backend=bibtex,
%    isbn=false,
%    url=false,
%    doi=false,
%    eprint=false,
%    style=numeric,
%    sorting=nyt,
%    sortcites = true
%]{biblatex}
%\bibliography{CT_Pipeline_l}
%\bibliography{CT_Skull_Stripping}


\usepackage{subfig}


<<label=opts, results='hide', echo=FALSE, message = FALSE, warning=FALSE>>=
library(knitr)
knit_hooks$set(webgl = hook_webgl) 
opts_chunk$set(echo=FALSE, prompt=FALSE, message=FALSE, warning=FALSE, comment="", results='hide')
@

<<label=setup, echo=FALSE >>=
rm(list=ls())
library(cttools)
library(fslr)
library(oro.dicom)
library(bitops)
library(arules)
library(plyr)
library(reshape2)
library(ggplot2)
library(matrixStats)
library(gridExtra)
library(qdap)
options(matlab.path='/Applications/MATLAB_R2013b.app/bin')

# username <- Sys.info()["user"][[1]]
rootdir = path.expand("~/CT_Registration")

ROIformat = FALSE
study = "Original_Images"
if (ROIformat) {
  study = "ROI_images"
}

basedir = file.path(rootdir, "Final_Brain_Seg")
resdir = file.path(basedir, "results")
paperdir = file.path(basedir, "Skull_Strip_Paper")
figdir = file.path(paperdir, "figure")

homedir <- file.path(basedir, study)

rdas = list.files(path=homedir, pattern=".*_CT_.*Header_Info.Rda", 
                  full.names = TRUE, recursive = TRUE)
stopifnot(!any(grepl("antry", rdas)))
rdas = rdas[grepl("Sorted", rdas)]
rda = data.frame(rda=rdas, stringsAsFactors = FALSE)
rda$id = basename(rda$rda)
rda$id = gsub("(.*)_Header_Info.*", "\\1", rda$id)


fname = file.path(resdir, "Overlap_Statistics.Rda")
load(fname)

img.id = unique(basename(ddf$img))
img.id = nii.stub(img.id)
rda = rda[ rda$id %in% img.id, ]
rownames(rda) = NULL

x = sapply(rda$rda, function(x){
  load(x)
  st = dcmtables[, "0018-0050-SliceThickness"]
  ust = unique(st)
  lust = length(ust)
if (lust > 1){
  print(ust)
}
  lust
})

n.var.slice = sum(x > 1)

proper = function(mystr) {
  x= strsplit(mystr, " ")[[1]]
  paste(toupper(substr(x, 1, 1)), tolower(substring(x, 2)),
        sep= "", collapse=" ")
}
uid = unique(basename(ddf$img))
nscans = length(uid)
num_scans = proper(replace_number(nscans))

pid = gsub("(\\d\\d\\d-(\\d|)\\d\\d\\d).*", "\\1", uid)
pid = unique(pid)
npt = length(pid)
ctr = unique(gsub("(\\d\\d\\d)-.*", "\\1", uid))
n.ctr = length(ctr)

ddf = ddf[ !grepl("refill", ddf$ssimg), ]

cs =  sapply(ddf, class) == "list"
cs = names(cs)[cs]
for (icol in cs){
  ddf[, icol] = unlist(ddf[, icol])
}

d = ddf
d$truevol = d$estvol = NULL
makeint = function(data){
  data$scen = gsub(".*_SS_(.*)_Mask.*", "\\1", data$ssimg )
  data$smooth = !grepl("nopresmooth", data$scen)
  data$smooth = revalue(as.character(data$smooth), 
                     c("TRUE"="Smoothed", "FALSE"="Unsmoothed"))
  data$int = gsub("_nopresmooth", "", data$scen)
  data
}
ddf = makeint(ddf)
ddf$diffvol = (ddf$truevol - ddf$estvol) / 1000
ddf$absdiff = abs(ddf$diffvol)

long = melt(d, id.vars = c("id", "img", "rimg", 
	"ssimg"))

long = makeint(long)
long$id = as.numeric(factor(long$id))

runcols =  c("dice", "jaccard", "sens", "spec", "accur", "absdiff")
rc = runcols[ !runcols %in% c("absdiff")]


wmax = function(x){
	which(x == max(x))
}
x = ddf[ ddf$img == ddf$img[1], ]


res = ddply(ddf, .(img), function(x){
	print(x$id[1])
	xx = sapply(x[, rc], wmax)
	xx = x$scen[xx]
	print(xx)
	names(xx) = rc
	xx
})

results= sapply(res[, rc], table)
maxtab = sapply(results, function(x) {
	names(sort(x, decreasing=TRUE)[1])
})

res = ddply(ddf, .(scen), function(x){
	cmin = colMins(x[, runcols])
	cmax = colMaxs(x[, runcols])
	cmean = colMeans(x[, runcols])
	cmed = colMedians(as.matrix(x[, runcols]))
	xx = data.frame(t(cbind(cmin, cmax, cmean, cmed)))
	xx$run = c("min", "max", "mean", "median")
	xx
})


nospec = long[ long$variable %in% c("accur", "sens"),]

long = long[ long$variable != "jaccard", ]

long$variable = revalue(long$variable, c("sens" = "Sensitivity",
                         "spec" = "Specificity",
                         "accur" = "Accuracy", 
                         "dice" = "Dice Similarity Index"))



@



<<tests>>=

mytest = function(...){
  wilcox.test(...)
}
all.smooth.tests = ddply(long, .(int), function(df){
  p.value= ddply(df, .(variable), function(x){
    stats = lapply(list(median, mean, sd), function(func){
      res = aggregate(value ~ smooth, func, data=x)
    })
    names(stats) = c("median", "mean", "sd")
    cn = stats[[1]]$smooth
    stats = lapply(stats, function(x){
      x$smooth = NULL
      x = c(t(x))
      names(x) = cn
      x
    })
    stats = unlist(stats)
    wt = wilcox.test(value ~ smooth, data=x, paired=TRUE)
    tt = t.test(value ~ smooth, data=x, paired=TRUE)
    return(c(wt.p.value=wt$p.value, tt.p.value = tt$p.value,  stats))
  })
}, .progress="text")


all.int.tests = ddply(long, .(smooth), function(df){
  df = df[df$int %in% c("0.01", "0.1"), ]
  p.value= ddply(df, .(variable), function(x){
    stats = lapply(list(median, mean, sd), function(func){
      res = aggregate(value ~ int, func, data=x)
    })
    names(stats) = c("median", "mean", "sd")
    cn = stats[[1]]$int
    stats = lapply(stats, function(x){
      x$int = NULL
      x = c(t(x))
      names(x) = cn
      x
    })
    stats = unlist(stats)
    wt = wilcox.test(value ~ int, data=x, paired=TRUE)
    tt = t.test(value ~ int, data=x, paired=TRUE)
    return(c(wt.p.value=wt$p.value, tt.p.value = tt$p.value,  stats))
  })
}, .progress="text")

smooth = all.int.tests[ all.int.tests$smooth == "Smoothed",]
pval <- function(pval) {
  x <- ifelse(pval < 0.001, "< 0.001", sprintf("= %03.3f", pval))
	return(x)
}
smooth$wt.p.value = pval(smooth$wt.p.value)
num = sapply(smooth, class) == "numeric"
smooth[, num] = round(smooth[, num], 4)
rownames(smooth) = smooth$variable
smooth$variable = smooth$smooth = NULL
smooth.dice = smooth["Dice Similarity Index",]
@

%\usepackage[all]{hypcap}
\begin{document}
\renewcommand{\thesubfigure}{\Alph{subfigure}}

\vspace{-1cm}
\paragraph*{Introduction}
Computed Tomography (CT) imaging of the brain is commonly used in diagnostic settings.  Although CT scans are primarily used in clinical practice, they are increasingly used in research.  A fundamental processing step in brain imaging research is brain extraction, which is the process of separating the brain tissue from all other tissues. Methods for brain extraction in head CT images has been informally proposed \citep{able, rorden_age-specific_2012}, but never formally validated.

Our goal is to systematically analyze the performance of the brain extraction tool (BET) \citep{smith_fast_2002}, a function of the FMRIB software library (FSL) \citep{jenkinson_fsl_2012}, on head CT images of patients with intracranial hemorrhage by varying parameters of BET and the use of smoothing after performing CT-specific preprocessing, and quantitatively comparing the results to the manual gold standard.

\vspace{-0.4cm}
\paragraph*{Methods}
\Sexpr{num_scans} images from \Sexpr{npt} patients with intracranial hemorrhage were selected from \Sexpr{n.ctr} different MISTIE (Minimally Invasive Surgery plus recombinant-tissue plasminogen activator for Intracerebral Evacuation) stroke trial centers. After NIfTI conversion, the data is thresholded to tissue ranges of $0$-$100$ Hounsfield units (HU).  In one variant of the processing pipeline, the data was smoothed using a Gaussian kernel ($\sigma=1$mm); in the other, the data was not smoothed.  BET was then applied to the image using 3 different fractional intensity (FI) values: $0.01$, $0.1$, or $0.35$ and then holes in the brain mask produced by BET were filled.
For validation purposes, intracranial masks were manually created for all image volumes by expert CT readers.  The resulting brain tissue masks were quantitatively compared to the manual segmentations using sensitivity, specificity, accuracy, and the Dice Similarity Index (DSI).  Brain extraction across smoothing and FI thresholds were compared using the Wilcoxon signed-rank test.

\vspace{-0.4cm}
\paragraph*{Results}
Figure~\ref{fig:metrics}\protect\subref*{unsmoothed} illustrates the performance of each variation of the BET pipeline.  The pipelines using smoothing (top) perform better than the unsmoothed pipelines (bottom) on all measures except specificity (all $p < 0.01$, uncorrected).  We also note that BET performs poorly on some scans without smoothing.  Figure~\ref{fig:metrics}\protect\subref*{smoothed} displays the performance for brain extraction for the pipelines using smoothed images. 

Using an FI of $0.01$ or $0.1$ performed better than $0.35$; thus, we focused and compared results for these values of FI only for the case when BET was applied to smoothed images.  Using an FI of $0.01$ had a higher median sensitivity ($\Sexpr{ smooth["Sensitivity", "median.0.01"] }$) than an FI of $0.1$ ($\Sexpr{ smooth["Sensitivity", "median.0.1"] }$, $p\Sexpr{ smooth["Sensitivity", "wt.p.value"] }$), lower specificity ($\Sexpr{ smooth["Specificity", "median.0.01"] }$ vs. $\Sexpr{ smooth["Specificity", "median.0.1"] }$; $p\Sexpr{ smooth["Specificity", "wt.p.value"] }$), with no difference in accuracy ($\Sexpr{ smooth["Accuracy", "median.0.01"] }$ vs. $\Sexpr{ smooth["Accuracy", "median.0.1"] }$; $p\Sexpr{ smooth["Accuracy", "wt.p.value"] }$) or DSI ($\Sexpr{ smooth["Dice Similarity Index", "median.0.01"] }$ vs. $\Sexpr{ smooth["Dice Similarity Index", "median.0.1"] }$; $p\Sexpr{ smooth["Dice Similarity Index", "wt.p.value"] }$).  Overall, regardless of p-values, these measures are all very high, indicating that multiple choices of parameters work well for brain extraction after CT image processing.
<<figcap_CT_Skull_Stripping_Figure2>>=
CT_Skull_Stripping_Figure2 = paste0("{\\bf Performance Metric Distribution for Different Pipelines.} ", 
"Panel~\\protect\\subref*{unsmoothed} displays boxplots for performance measures when running the pipeline with different FI, using smoothed data (top) or unsmoothed data (bottom).   Using smoothed data improves performance in performance metrics.  Panel~\\protect\\subref*{smoothed} presents the smoothed data only-- using an FI of $0.01$ and $0.1$ perform better than $0.35$ in all categories other than specificity. Based on all metrics, we recommend smoothing and an FI of $0.01$." )
@
\vspace{-0.5cm}
\begin{SCfigure}[][h]
  \subfloat{
  \label{unsmoothed}
\includegraphics[width=.33\textwidth]{figure/CT_Skull_Stripping_Figure2.png}
}
\hfill
  \subfloat{
  \label{smoothed}
\includegraphics[width=.33\textwidth]{figure/CT_Skull_Stripping_Figure2b.png}
}
\caption{\Sexpr{CT_Skull_Stripping_Figure2}}
\label{fig:metrics}
\end{SCfigure}

\vspace{-0.5cm}
\paragraph*{Conclusion}
We have introduced the first validated automated brain extraction pipeline for head CT images.  A novel finding is that smoothing the data using a conservative smoother ($1$mm Gaussian kernel) and using an FI of $0.01$ or $0.1$ provides good brain extraction for the sample studied.  These choices make a large difference in the performance of the algorithms and have not been previously reported in the literature.  The research presented here is fully reproducible and we provide ready-to-use software for CT brain extraction (see \url{http://bit.ly/CTBET_RCODE}). Our software is readily available to all users as it is based on open-source, free programs (FSL and R).


\vspace{-0.4cm}
\paragraph*{Sources of Funding}
The project described and data used were supported by the NIH grants RO1EB012547, T32AG000247, R01NS046309, RO1NS060910, RO1NS085211, R01NS046309, U01NS080824, U01NS080824 and U01NS062851 and RO1MH095836.

\vspace{-0.25cm}
\subsubsection*{References}
\renewcommand\refname{\vskip -1cm}
\bibliographystyle{abbrv}
\bibliography{CT_Skull_Stripping_Abstract}

%\printbibliography



\end{document}